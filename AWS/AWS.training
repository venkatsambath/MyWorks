AMI is packaged invironment to boot EC@ instance

Specify AMI when launching instance

Have one or many

Includes the following:

a. Template for root volume
b. Launch permission - which instance can use the AMI?
c. Specify the volumn to attach to the instance - Block device mapping

Difference between PV(para virtualinstance) and HVM(Hardware assisted vitual instance)
most average Amazon (AWS) users could effectively ignore the distinction between paravirtual instances (PV) and hardware assisted virtual instances (HVM). HVM was a technology reserved for high-performance, large-capacity use cases. EG: 10Gbe LAN connection, >64 GB RAM, etc. But with changes being made in AWS, all users can and should be considering HVM for their instances.

Elastic Loadbalancing:

ELB helps you achieve fault tolerance by allowing you to distribute traffic across multiple EC2 instances and availability zones.

Here we see a representation of an ELB routing traffic to two availability zones, but we know these two availability zones are actually geographically separated physical locations within a region. Our single CNAMEd ELB component actually resolves round robin DNS to ELB IP addresses in each availability zone and as traffic increases, AWS adds IP addresses to ELB's DNS entry and continues to round robin requests across the multiplying ELBs.

Of course, as traffic decreases it removes the IP addresses from the ELB's DNS entry thereby reducing the number of load balancing components in the system. Design for Failure Lesson Number Four: Use Elastic Load Balancing to easily distribute your system across multiple resources and availability zones to ensure your system remains up and running even if a single resource fails or disaster strikes.


Cloudwatch[free of charge]:

Cloudwatch allows you to monitor resources immediately and automatically without having to install or configure any software. It allows you to have visibility in the resource utilization performance and traffic load demand patterns. It allows you to gather and graph resource metrics such as CP utilization, disk I/O and network traffic. It allows you to set alarms when specific metric thresholds are breached, which can then trigger an action such as sending notifications or even kick-off a process to automatically handle the issue such as add or remove another resource.

By utilizing a simple put API request you can define and submit custom metrics generated by your own application and have them monitored by Cloudwatch. Here is a list of all the services that currently support Cloudwatch monitoring If some of these service abbreviations are new to you, don't worry because we're going to be covering a great many of these in subsequent sections. As mentioned, most of these allow you to use CLoudwatch for free with the option to pay for more detailed monitoring.

If you opt to pay a little more for detailed monitoring, you get monitoring at a higher frequency of one-minute intervals versus the five-minute intervals in the free plan as well as being able to have access to a larger number of pre-defined metrics and to monitor aggregate metrics across groups of similar resources. Design for Failure Lesson Number Five: Utilize Amazon Cloudwatch to get more visibility and take appropriate actions in case of hardware failure or performance issues.

Types of Volume Storage:

 Elastic block storage, which persists after an instance is terminated And ephemeral volumes or local storage, which does not survive termination.

 Elastic block storage or EBS are storage resources that you create separately from EC2 instances. You then attach the EBS volumes to EC2 instances. Once attached, you can use them just as you would any other block device. For example, you can create a file system or run a database on top of these storage devices. They also come in two types. There is Standard volumes and Provisioned IOPS volumes. IOPS stands for input/output operations per second.

And Provisioned IOPS EBS volumes allow you to control and specify consistent performance parameters when creating volumes. Standard and Provisioned IOPS volumes differ in price and performance. And choosing one over the other will depend on your specific use cases and budget. With EBS, you can create volumes up to 1 terabyte in size and attach them to EC2 instances. You can also attach multiple volumes to a single instance.

You can specify I/O performance by creating volumes with Provisioned IOPS. You can format Amazon EBS volumes with a file system and load them with applications and data or use them in any other way you would use a block device. You can create point-in-time snapshots and have these snapshots persisted to Amazon S3. You can use these snapshots to instantiate new volumes. You can then take these snapshots and copy them across AWS regions making it easier to take advantage of AWS regions for systems that require large geographical distribution.

And you can use Cloudwatch to view performance metrics for Amazon EBS volumes giving you insight into metrics such as throughput and latency. I just mentioned on the previous slide that EBS allows you to take point-in-time snapshots of your volumes. This is an important feature, especially with regard to designing for failure. EBS snapshots are easy to take, either through the web console or programmatically through the API. You can even take a snapshot of a volume that is currently in use by an instance.

However, snapshots only capture data that has been written to your Amazon EBS volume at the time the snapshot command is issued. If you can pause any writes to the volume long enough to take a snapshot, your snapshot should be complete. However, if you can't pause all writes to the volume, you should consider unmounting the volume from within the instance. Then, issue the snapshot command and then remount the volume to ensure a consistent and complete snapshot. You can remount and reuse the volume while the snapshot status is still in a pending state.

Amazon snapshots are stored incrementally, which means only the blocks that have changed after your last snapshot are saved. And you are only billed for the changed blocks. If you have a device with 100 gigabytes of data, but only 5 gigabytes is changed after your last snapshot, a subsequent snapshot consumes only 5 additional gigabytes. And you're billed only for the additional 5 gigabytes of snapshot storage. When you delete a snapshot, you remove only the data needed by any other snapshot.

All active snapshots contain all the information needed to restore the volume to the instant at which the snapshot was taken. Snapshots can be used to create new volumes or change attributes of existing ones. Like expanding the size of a volume or moving volumes across availability zones or sharing them across regions. The following are some key features of Amazon EBS snapshots. You have immediate access to restored snapshots. As has been mentioned, snapshot data is stored in S3.

If you create a volume from a snapshot, all of this data stored in S3 will have to be transferred to your newly created volume. However, if you want to start using the volume with one of your instances right away, you can. The restoring of new volumes from EBS snapshots implements a lazy loading approach. So, any data that you start to access will be prioritized in the transfer. And if not already retrieved, will be immediately retrieved upon the first request, so you can begin using them right away.

Snapshots are also useful for resizing your volumes. When you create a new Amazon EBS volume based on a snapshot, you may change the size specified for the new volume. You will want to double check that your file system and/or your application supports such a resizing. Snapshots are also easy to share. Amazon EBS snapshots make it easy for you to share your data, say with your coworkers or others in the AWS community. Users can create their own Amazon EBS volumes based on the EBS snapshots you decide to share.

And snapshots are also easy to share across AWS regions. The ability to copy snapshots across regions makes it easier to take advantage of using AWS regions for geographical distribution. So, Design for Failure, lesson number 6. Utilize EBS and take advantage of incremental snapshots which are automatically uploaded to S3, and keep your persistent data independent from your EC2 instances.

Bootstrapping:

 An example bootstrap process could be as follows, we could mount needed drives and start any needed services, update code files or pull down needed scripts, get latest version of supporting software or apply any needed software patches, and then perhaps register itself with a load balancer and start receiving traffic.

The tools you'll be using to achieve this bootstrapping depends on your application environment and the existing tools you might already be using for configuration and deployment management. Some examples are running custom scripts on the servers. When an instance starts up, you can run your custom scripts that may set configuration settings, start services, apply software updates, etc., and to aid in this effort, AWS helps out in a couple of ways, which we'll discuss on the next slide. You may already be using configuration management tools, such as Chef or Puppet.

If so, these will probably continue to play a role in your bootstrap process. Opsworks is another service available in the AWS landscape that is a full-blown application management service which is integrated into AWS' services suite and provides a way to manage all aspects of your application life cycle. It is built on top of the Chef framework. So, if you're familiar with Chef, Opsworks might be a good fit. Another advantage of Opsworks is that Amazon offers this service for free.

So, it's something to check into if you're looking for a full configuration management solution. I wanted to also mention a couple things that AWS offers with regard to running your own custom scripts as part of the bootstrapping process. One thing is access to your instance metadata. You have access to instance data directly from within an instance. You can access this data directly from the command line. So, let me take just a moment to demonstrate this. So, here I have SSH'd into one of my EC2 instances, and this one is based on an Amazon Linux AMI.

Don't worry how I got here at this point. I'm going to be showing you all of this later in the course. For now, I just wanted to show you this instance data. All EC2 instance data is available at the same endpoint over http. On a Linux instance, you can use something like curl or get, if your environment supports it, to view the data. Here, I'll use curl and list out the available metadata options. So, from the command line I can type in curl, http, colon, slash, slash, 169 dot 254 dot 169 dot 254 and I'm going to get the latest metadata, the latest version of metadata, and then just metadata, and what returns is a listing of all the instance metadata that's available to me.

It's quite a long list. We can take a look at just a few of them. For example, say I want to get the instance's host name. Well, I can use that same URL endpoint and just type in host name, and let me echo out also a new line, just for readability. There we have the host name. I could also take a look at, for example, the unique AMI ID from which this instance was created. I use the same endpoint again, but this time I'll use AMI ID, and there it is.

And then, for example, I could also look at the unique instance identifier that this instance has associated with it. With that, that is just instance dash ID. So, you can get access to all of these that were mentioned before at the top by simply accessing this same endpoint from anywhere on your instance. You can also access these values from within scripts that are run on your instances, and we'll be writing a short PHP script later in the course to retrieve and display the instance ID.

AWS also offers something called Cloud-Init for Linux servers or EC2Config for Windows servers that allow you to easily specify the scripts or any shell commands you need to run upon startup. This makes it easy to specify server bootstrap needs, and they will be automatically run at the appropriate time during the overall instance startup life cycle. When I create new EC2 instances later in this course, I will be using Cloud-Init to script the setup of a new LAMP stack server and demonstrate the use of user data and Cloud-Init to bootstrap a new instance.

So, implementing elasticity lesson number one, take the time to write the scripts or configure management tools necessary to bootstrap your instances. Automating this process will allow you to take advantage of the Cloud's elastic nature.

Auto Scaling:

- As we discuss implementing elasticity, few AWS services could provide more benefit in this regard than Auto-Scaling. Auto-Scaling is a service focused on helping you implement elasticity. Remember when I first talked about understanding elasticity, and we used some graphs that demonstrated the issues that can occur with not allocating enough capacity? And the waste that is generated by allocating too much capacity in traditional environments? If you recall, the solution was to take advantage of the elastic nature of the cloud.

And to increase capacity exactly when load demands, and to decrease capacity when load lightens, always matching the right amount of capacity to meet demand. Auto-Scaling is a service provided by Amazon that makes this easy. You define the conditions on which you want to scale out, or in, your EC2 instances. And the Auto-Scaling service will add, or remove, instances when these conditions are met. For example, let's say you have a web application architected as depicted here.

Here the application is distributed across two Availability Zones because we have designed for failure, and know that doing so achieves a high degree of fault tolerance. During periods of normal to light load, maybe throughout the night and early morning, your web application works fine with just these two servers. Now, a little later in the morning, when everyone is grabbing their coffee and checking out your great new app, traffic starts to increase. Since we are using CloudWatch to monitor our servers, we know when the CPU levels on the boxes rise.

We can use Auto-Scaling and define the condition, such that, if that average CPU utilization rises above 75%, say, for more than a minute, add two more instances. One in Zone A, and another in Zone B. So without us having to lift a finger the Auto-Scaling service has added two more instances and our average CPU drops down a little. If it at any other time throughout the day they rise above 75% again, two more instances will be added, scaling the system out once again, to accommodate the increase in demand.

Then, as the day turns into evening, and everyone logs off your great web app in order to cook some dinner and head to bed, the load on your servers begins to diminish. And since we have also used Auto-Scaling to set up a condition, such that, if the average CPU utilization drops below 35%, remove two instances. One from Zone A and one from Zone B. And this will continue to happen until it gets back down to a defined minimum number of instances, which, in our case, is two. Now we are back at our two servers that will easily get us through the night.

Our app users are happy, because we met all of their demands and we're only using what we need, and only paying for what we use. So everyone wins. Auto-scaling has three primary components to it. The first part is what is called the launch configuration and this defines what to scale. This is where you define what EC2 instance size to use, what AMI to use, your security group, storage needs, et cetera. Very much the same as all the stuff you have to define to launch a new instance.

The second part is called an Auto-Scaling group. This defines the "where" we want to launch instances and also allows us to define limits on the number of instances to launch, should certain events occur. One of these is a desired capacity number. And Auto-Scaling will work to keep your number of instances equal to your desired capacity. So with just these two components defined Auto-Scaling will provide you with some fault tolerance. The third component is optional, but is really important to fully implement the elasticity we talked about in our example.

It is called a scaling policy and it defines the "how" instances are launced. This where you can define CloudWatch alarms based on certain metrics breaching specified thresholds. Referring to our initial example, it is in the scaling policy where we would define the condition to have two more instances added, if the average CPU utilization rose above 75%, and to have two instances removed if that same metric fell below 35%.

It is a good idea to define both a scale-out and a scale-in policy, in order to fully take advantage of elasticity. You want to make sure you scale out to handle increase in load, but you also want to scale back in to keep costs low and avoid waste. So, Implementing Elasticity Rule Number Two: Take advantage of the Auto-Scaling service to greatly simplify the process of automating your scaling. This helps keep your application users happy, and your business costs minimized.

IAM

As we saw in the last section while learning about the shared security model, there was one area that remained the responsibility of the customer regardless of the service category. This was the customer account and IAM permissions and access configuration. IAM which is short for identity and access management is a management tool that allows you to control access and permissions to the AWS resources and services in your account. Here are the basics of IAM. When you first create a new AWS account, what you have created is the master account, and the user associated with this account is the master user.

This is like root user on Unix-based operating systems. And as such, has access to everything including all of your billing information. So, it makes sense to keep this master account secured. It is recommended not to use this master account for accessing your services and resources, but rather set up the appropriate access control structures through IAM. It is also recommended that you set up multi-factor authentication on this account for added security. With IAM, you manage access control entities you are probably already familiar with.

Such as users, groups, roles, and permissions. You create these and then apply them to individual services and API calls, which allow you to define permissions that control which users can access which specific services, which actions they can perform within these services, and which resources are available within these services. Within your master account, you use IAM to create users. When you create a new user, the new user has no access granted by default.

This adheres to the security principle of least privilege, which specifies that a user or resource should only have the minimum permissions necessary to carry out and perform their responsibilities. Having the user start with absolutely no permissions, allows you to add only those required for the user to fulfill their specific tasks. There are different credential types that user may need. If they need access to the console, they will need login credentials. And if they'll be using the API to interface with services, they will need an access key.

You then assign privileges either individually to the user or by placing the user within a group that has certain permissions defined, which brings us to the next part groups. Within your master account, you also create groups. Groups are composed of a set of one or more privileges, and then you add users to these groups, which allows you to manage permissions on a group level rather than on an individual by individual basis. You can also create roles. When you create a role, you assign one or more access permissions to that role.

IAM roles allow users or services to act on behalf of the account owner. But that otherwise don't normally have access to your services or resources. IAM users or AWS services can assume a role to obtain temporary security credentials that can be used to make AWS API calls. By using roles, you don't have to share long-term credentials or define permissions for each entity that requires access to a resource. Roles also simplify the permissions needed by applications running on EC2, which may require access to other AWS resources.

To grant applications on an EC2 instance access to AWS resources, you can create the needed role with the required permissions and then launch the EC2 instance itself into that role. This then embeds the needed access keys into the instance metadata. And applications can access this metadata on the instance to retrieve the access they need. Allowing applications to assume these roles, provides better security than providing long-term dedicated access credentials to each application.

We will be using IAM a little more when we start using AWS in the next chapter. So, Keeping Things Secure, lesson number two. Never use your master account to access your resources or manage your services. Create separate users, groups, roles, and permissions in IAM and only allow access when absolutely needed and only to the specific resources that are needed.


VPC:

Okay. The last topic to introduce as we are covering how to keep things secure is Amazon's Virtual Private Cloud or simply VPC. We've mentioned VPC before in talking about AWS services. In fact, just in the previous section, we singled out VPC as an exception to the statement that security groups only control inbound traffic. So, this is not the very first time you've heard its name, but what is it? Well, let's start by talking briefly about the platform referred to as EC2 Classic.

EC2 Classic is the original release of EC2 before VPC was introduced. When EC2 first launched, your instances would run in a single flat network that is shared with other AWS customers. Whenever you launched an instance, it would automatically be associated with a public IP and a private IP address. Furthermore, every single EC2 instance you would launch would be internet addressable. So, with EC2 Classic, if you wanted to launch an EC2 instance for the purpose of hosting your database, that instance would have a public internet host name and IP whether you wanted it to or not.

And typically your database server has no business being out on the global internet. There is also no distinction between a private and public network interface with instances in EC2 Classic. So, disabling all traffic on the public host name is not truly an option. These are some of the reasons that EC2 Classic is considered less secure. It's not that you can't achieve a similar security level with these restrictions, but it can be more challenging and you are limited in what you can do.

So, I think I can hear all of the IT and network security professionals out there yelling, "There's got to be a better way." Amazon must have heard you as well because they have answered with VPC, the new way to launch your EC2 instances. With EC2-VPC, instances run in a virtual private cloud that is logically isolated to only one AWS account. Your EC2 nodes launched into a VPC are not automatically addressable via the public internet.

And you can control both the private and public network interfaces. But wait, there is more. With VPCs, you really start to mirror the network setup you would outline if you are providing your own hosting solution. You specify your own private IP range and then architect this range into a combination of public and/or private subnets. Also, as was mentioned in the previous section on security groups, you now get to control both your inbound and outbound traffic.

You can assign multiple IP addresses, elastic IP addresses, and elastic interface networks to your instances. Also, if the use case arises, you can securely connect existing IT infrastructure not hosted on AWS with your VPC network through an encrypted VPN connection. With all of this flexibility and myriad features, comes a little bit of complexity. Setting up a VPC from scratch takes a little more work and thought on your part. Let's quickly walk through at a very high level, the steps you take when initially setting up a VPC.

First, you select the AWS region into which you want the VPC to reside. Then, you decide on the block of internal IP addresses you want to use for your VPC. Here, you want to think through your needs and ensure you provide a large enough range to support future growth. Now, you start creating your subnets within your VPC. The subnets are confined within an individual availability zone and don't span between them. Each of these subnets themselves have a block of IP addresses to find via the CIDR notation and would be a subset of the block defined for the entire VPC.

Next, you start attaching gateway interfaces to allow access to your network. Here, we have decided to attach an internet gateway to Subnet 2 to allow it to have access to the public internet, thereby making it a public subnet. And as we briefly mentioned earlier, you can use a VPN gateway to allow your subnets to route to your company hosted infrastructure over a secure VPN connection. This is barely the tip of the VPC iceberg. There was a lot more we could talk about on the topic, but we need to move onto the part of the course where we finally start using AWS and start to see some of this in action.

But know that a VPC offers more security options than EC2 Classic and that EC2 Classic is now deprecated in favor of the VPC approach. In fact, with brand new accounts, you can only launch instances into VPC. However, there is a default VPC to work within, which still makes getting started very quick and easy. So, Keeping Things Secure, lesson number 4. Become familiar with VPCs and take the time to properly set up a VPC for your environment to fully take advantage of all the new security options available.


